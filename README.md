# Electric Bus Charging Coordination with Q-Learning

![Electric Bus Simulation](results/figures/Comparison_of_EV_Charging_Strategies.png)

Official implementation of **"Decentralized Charging Coordination for Electric Buses: A Q-Learning Approach"** - a reinforcement learning solution for optimizing electric bus charging in urban transit systems.

## ğŸ“‹ Overview

This project addresses the critical challenge of coordinating charging for electric bus fleets operating on fixed urban routes. By formulating the problem as a Markov Decision Process and applying tabular Q-learning, we develop a decentralized policy that significantly reduces charging wait times while maintaining safe battery levels.

## ğŸš€ Key Results

| Metric | Baseline | Tuned Heuristic | **RL Agent** | Improvement |
|--------|----------|-----------------|--------------|-------------|
| **Avg. Wait Time** | 729.2 Â± 187.0 s | 373.3 Â± 102.2 s | **178.8 Â± 82.0 s** | **75.5%** vs Baseline |
| **Avg. State of Charge** | 55.1% | 54.9% | **42.2%** | More efficient utilization |
| **Statistical Significance** | p < 0.001 | p < 0.001 | - | Robust performance |

## ğŸ—ï¸ System Architecture

The simulation models Shenzhen Bus Line 303 with:
- **12 BYD K9 electric buses** (350 kWh battery capacity)
- **5 charging stations** (dual 120 kW ports each)
- **84.2 km round-trip route**
- **180-minute simulation** with 5-second time steps

## ğŸ§  Reinforcement Learning Approach

### State Space
```
s = (SOC_bucket, dist_bucket, time_factor, câ‚, câ‚‚, câ‚ƒ, câ‚„, câ‚…)
```
- **SOC_bucket**: Battery state-of-charge (6 discrete levels)
- **dist_bucket**: Distance to nearest charger (5 levels)
- **time_factor**: Time-of-day indicator (4 periods)
- **cáµ¢**: Charger status triple (availability, queue, utilization)

### Action Space
```
A = {0, 1, 2, 3, 4}  # Select one of 5 charging stations
```

### Reward Function
```python
R(s,a,s') = 100Â·I{connection} + 300Â·Î”SOC - 20Â·distance
           - penalty(SOC) - penalty(queue)
```

## ğŸ“ Project Structure

```
electric-bus-rl/
â”œâ”€â”€ src/                    # Simulation source code
â”‚   â”œâ”€â”€ bus.py             # Bus dynamics and energy model
â”‚   â”œâ”€â”€ charging_station.py # Charging infrastructure
â”‚   â”œâ”€â”€ simulation.py      # Main simulation loop
â”‚   â”œâ”€â”€ rl_agent.py       # Q-learning implementation
â”‚   â””â”€â”€ config.py         # Simulation parameters
â”œâ”€â”€ results/               # Key outputs
â”‚   â”œâ”€â”€ figures/          # Generated plots
â”‚   â””â”€â”€ tables/           # Statistical results
â”œâ”€â”€ paper/                 # Paper materials
â”‚   â””â”€â”€ latex/            # LaTeX source files
â”œâ”€â”€ scripts/               # Utility scripts
â””â”€â”€ docs/                  # Documentation
```

## ğŸ› ï¸ Installation & Usage

### Prerequisites
```bash
python >= 3.8
pip install -r requirements.txt
```

### Quick Start
```python
# Run a simulation with RL agent
from src.simulation import Simulation
from src.rl_agent import RlAgent

sim = Simulation(config_path='config/shenzhen_line_303.json')
agent = RlAgent()
results = sim.run(agent, duration_minutes=180)
```

### Training the Agent
```bash
python scripts/train_agent.py --episodes 200 --save_path models/q_table.npy
```

### Evaluating Strategies
```bash
python scripts/run_comparison.py --strategies baseline heuristic random rl
```

## ğŸ“Š Performance Comparison

![Training Progress](results/figures/RL_Training_Wait_Time_Progression.png)
![SOC Dynamics](results/figures/State_of_Charge_Over_Time.png)

**Key Insights:**
1. **Anticipatory Charging**: RL agent initiates charging at 35-38% SOC (vs 40% threshold)
2. **Load Balancing**: Distributes buses across stations to reduce congestion
3. **Safety-Conscious**: Maintains all buses above 30% SOC safety limit

## ğŸ“ˆ Results Analysis

The Q-learning agent demonstrates:
- **75.5% reduction** in average wait time compared to nearest-charger baseline
- **52.1% improvement** over carefully tuned heuristic
- **Lower average SOC** (42.2% vs ~55%) indicating efficient battery utilization
- **Reduced variability** in wait times (smaller confidence intervals)

## ğŸ”§ Configuration

Key simulation parameters in `src/config.py`:
```python
SIMULATION_CONFIG = {
    'num_buses': 12,
    'num_stations': 5,
    'ports_per_station': 2,
    'battery_capacity': 350,  # kWh
    'charging_power': 120,    # kW per port
    'consumption_rate': 1.65, # kWh/km
    'route_length': 84.2,     # km
    'soc_threshold': 0.4,     # Charging initiation
    'target_soc': 0.8,        # Charging target
}
```

## ğŸ“š Citation

If you use this code or reference our work, please cite:

```bibtex
@inproceedings{kaushik2025decentralized,
  title={Decentralized Charging Coordination for Electric Buses: A Q-Learning Approach},
  author={Kaushik, Diya},
  booktitle={IEEE Conference Proceedings},
  year={2025},
  pages={1--8}
}
```

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- Computing resources provided by Galgotias University
- Faculty members of Department of Computer Science and Engineering
- Dr. Suveg Moudgil for guidance and mentorship

## ğŸ“¬ Contact

**Diya Kaushik**  
Department of Computer Science and Engineering  
Galgotias University  
Gautam Buddha Nagar, India  
[diyakaushik027@gmail.com](mailto:diyakaushik027@gmail.com)


---

**Related Papers:** [Electric bus charging station placement with queueing considerations](https://doi.org/10.1016/j.trc.2019.01.020) | [Optimal charging scheduling for fast-charging bus systems](https://doi.org/10.1016/j.tre.2019.01.002)

**Tags:** `reinforcement-learning` `q-learning` `electric-buses` `smart-cities` `public-transportation` `charging-coordination`
